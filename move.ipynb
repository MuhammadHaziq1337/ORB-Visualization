{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Assuming all files are in the current directory, adjust paths as necessary\n",
    "utterances_file = 'movie/utterances.jsonl'\n",
    "speakers_file = 'movie/speakers.json'\n",
    "conversations_file = 'movie/conversations.json'\n",
    "output_csv_file = 'new_movie_dialogs.csv'\n",
    "\n",
    "# Load the speaker and conversation data into memory for quick lookup\n",
    "speakers_data = {}\n",
    "with open(speakers_file, 'r', encoding='utf-8') as file:\n",
    "    speakers_data = json.load(file)\n",
    "\n",
    "conversations_data = {}\n",
    "with open(conversations_file, 'r', encoding='utf-8') as file:\n",
    "    conversations_data = json.load(file)\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_csv_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['utterance_id', 'conversation_id', 'text', 'speaker_id', 'character_name', 'gender', 'movie_id', 'movie_name', 'release_year', 'rating', 'votes', 'genre']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Open the utterances file and read line by line\n",
    "    with open(utterances_file, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= 25000:  # Only process the first 150 lines\n",
    "                break\n",
    "            utterance_data = json.loads(line)\n",
    "            \n",
    "            # Extract speaker information using the speaker ID from the utterance\n",
    "            speaker_id = utterance_data.get('speaker')\n",
    "            speaker_info = speakers_data.get(speaker_id, {}).get('meta', {})\n",
    "            \n",
    "            # Extract conversation/movie information using the conversation ID from the utterance\n",
    "            conversation_id = utterance_data.get('conversation_id')\n",
    "            conversation_info = conversations_data.get(conversation_id, {}).get('meta', {})\n",
    "            \n",
    "            # Write the relevant information to the CSV\n",
    "            writer.writerow({\n",
    "                'utterance_id': utterance_data.get('id', 'N/A'),  # Provide a default if missing\n",
    "                'conversation_id': conversation_id or 'N/A',\n",
    "                'text': utterance_data.get('text', 'N/A'),\n",
    "                'speaker_id': speaker_id or 'N/A',\n",
    "                'character_name': speaker_info.get('character_name', 'N/A'),\n",
    "                'gender': speaker_info.get('gender', 'N/A'),\n",
    "                'movie_id': speaker_info.get('movie_idx', 'N/A'),\n",
    "                'movie_name': conversation_info.get('movie_name', 'N/A'),\n",
    "                'release_year': conversation_info.get('release_year', 'N/A'),\n",
    "                'rating': conversation_info.get('rating', 'N/A'),\n",
    "                'votes': conversation_info.get('votes', 'N/A'),\n",
    "                'genre': ', '.join(conversation_info.get('genre', []))  # Join the list into a comma-separated string\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           text entities\n",
      "0  They do not!       []\n",
      "1   They do to!       []\n",
      "2    I hope so.       []\n",
      "3     She okay?       []\n",
      "4     Let's go.       []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('new_movie_dialogs.csv')\n",
    "\n",
    "# Define a function to perform NER\n",
    "def extract_entities(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract entities from the doc\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    # Return the entities\n",
    "    return entities\n",
    "\n",
    "# Apply the NER function to the first 50 rows to keep computation time reasonable\n",
    "df['entities'] = df['text'].head(50).apply(extract_entities)\n",
    "\n",
    "# Now df['entities'] contains the named entities recognized in each piece of text\n",
    "print(df[['text', 'entities']].head())\n",
    "\n",
    "# You could then save this back to CSV if desired\n",
    "df.to_csv('movie_dialogs_with_entities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Find interactions\n",
    "interactions = df.groupby('conversation_id')['speaker_id'].apply(list)\n",
    "\n",
    "# Create nodes and links\n",
    "nodes = []\n",
    "links = []\n",
    "\n",
    "for conversation in interactions:\n",
    "    for i, speaker in enumerate(conversation):\n",
    "        if speaker not in nodes:\n",
    "            nodes.append(speaker)\n",
    "        if i < len(conversation) - 1:  # Check if there is a next speaker\n",
    "            links.append({'source': speaker, 'target': conversation[i + 1]})\n",
    "\n",
    "# Create DataFrames for nodes and links\n",
    "nodes_df = pd.DataFrame({'Character_ID': nodes})\n",
    "links_df = pd.DataFrame(links)\n",
    "\n",
    "# Create a DataFrame for interactions\n",
    "interactions_df = pd.DataFrame({'conversation_id': interactions.index, 'speakers': interactions})\n",
    "\n",
    "# Save interactions to a CSV file\n",
    "interactions_df.to_csv('interactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conversation_id\n",
       "L1007                           [u0, u5, u0, u5, u0]\n",
       "L101486                                 [u592, u585]\n",
       "L101494                                 [u585, u595]\n",
       "L101502                     [u595, u585, u595, u585]\n",
       "L101507               [u585, u592, u585, u592, u585]\n",
       "                             ...                    \n",
       "L982                                        [u5, u0]\n",
       "L984                                        [u0, u2]\n",
       "L986                                  [u11, u5, u11]\n",
       "L989       [u11, u5, u11, u5, u11, u5, u11, u5, u11]\n",
       "L998                                  [u11, u5, u11]\n",
       "Name: speaker_id, Length: 7001, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movie_dialogs_with_sentiment.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('new_movie_dialogs.csv')\n",
    "\n",
    "# Initialize the sentiment intensity analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score\n",
    "def get_sentiment(text):\n",
    "    # Since text can be NaN, we convert it to an empty string which is handled by VADER\n",
    "    text = str(text) if pd.notnull(text) else ''\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "# Apply the function to get sentiment score for each dialogue\n",
    "df['sentiment'] = df['text'].apply(get_sentiment)\n",
    "\n",
    "# Save the dataframe with the sentiment scores to a new csv file\n",
    "output_file = 'movie_dialogs_with_sentiment.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('new_movie_dialogs.csv')\n",
    "\n",
    "# Define a function to extract entities\n",
    "def extract_entities(text):\n",
    "    # Skip empty texts or NaNs\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "\n",
    "# Apply the function to extract entities for each dialogue\n",
    "df['entities'] = df['text'].apply(extract_entities)\n",
    "\n",
    "# Save the dataframe with the entities\n",
    "df.to_csv('movie_dialogs_with_entities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('movie_dialogs_with_entities.csv')\n",
    "# Calculate dialogue count and average sentiment per character\n",
    "character_metrics = df.groupby('speaker_id').agg({\n",
    "    'text': 'count',\n",
    "    'sentiment': 'mean'\n",
    "}).rename(columns={'text': 'dialogue_count', 'sentiment': 'average_sentiment'}).reset_index()\n",
    "\n",
    "# Extract entities and count the most common ones per character\n",
    "character_entities = (\n",
    "    df.explode('entities')\n",
    "    .groupby(['speaker_id', 'entities'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values(['speaker_id', 'count'], ascending=[True, False])\n",
    "    .groupby('speaker_id')\n",
    "    .head(5)  # Top 5 entities per character\n",
    ")\n",
    "\n",
    "# Merge the metrics and entities\n",
    "character_profile = character_metrics.merge(character_entities, on='speaker_id', how='left')\n",
    "\n",
    "# Save to CSV\n",
    "character_profile.to_csv('character_profiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.041*\"know\" + 0.022*\"yeah\" + 0.014*\"well\" + 0.010*\"way\" + 0.009*\"sorry\"')\n",
      "(1, '0.028*\"like\" + 0.020*\"right\" + 0.019*\"go\" + 0.016*\"got\" + 0.015*\"okay\"')\n",
      "(2, '0.024*\"yes\" + 0.019*\"one\" + 0.012*\"get\" + 0.012*\"tell\" + 0.012*\"got\"')\n",
      "(3, '0.014*\"know\" + 0.014*\"think\" + 0.012*\"us\" + 0.011*\"see\" + 0.010*\"like\"')\n",
      "(4, '0.025*\"want\" + 0.020*\"oh\" + 0.013*\"good\" + 0.012*\"time\" + 0.007*\"uh\"')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('new_movie_dialogs.csv')\n",
    "\n",
    "# Initialize a tokenizer and stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(str(text).lower())  # Convert to string to handle NaN\n",
    "    # Remove stopwords\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "df['processed'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(df['processed'])\n",
    "\n",
    "# Create a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df['processed']]\n",
    "\n",
    "# Perform LDA\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.LdaMulticore(doc_term_matrix, num_topics=NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "# Assign the most relevant topic to each document\n",
    "def get_dominant_topic(doc_bow):\n",
    "    topic_distribution = ldamodel.get_document_topics(doc_bow)\n",
    "    topic_distribution = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "    return topic_distribution[0][0] if topic_distribution else None\n",
    "\n",
    "df['dominant_topic'] = [get_dominant_topic(doc) for doc in doc_term_matrix]\n",
    "\n",
    "# Save the dataframe with the dominant topics to a new csv file\n",
    "output_file = 'movie_dialogs_with_topics.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print some of the topics for reference\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with entities and sentiment\n",
    "df = pd.read_csv('movie_dialogs_with_entities.csv')\n",
    "\n",
    "# Function to safely evaluate the string representation of lists\n",
    "def safe_eval_list(s):\n",
    "    try:\n",
    "        return eval(s)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the safe_eval_list function to the 'entities' column\n",
    "df['entities'] = df['entities'].apply(safe_eval_list)\n",
    "\n",
    "# Drop rows where 'entities' column is '[]' and 'sentiment' column is 0\n",
    "df = df[(df['entities'].str.len() > 0) & (df['sentiment'] != 0)]\n",
    "\n",
    "# Save the cleaned dataframe to a new csv file\n",
    "cleaned_output_file = 'cleaned_movie_dialogs.csv'\n",
    "df.to_csv(cleaned_output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
